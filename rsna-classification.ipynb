{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:19.689518Z",
     "start_time": "2020-05-16T00:12:19.683520Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import tensor\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:19.695516Z",
     "start_time": "2020-05-16T00:12:19.691517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:19.701514Z",
     "start_time": "2020-05-16T00:12:19.696516Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\".\", \"rsa-pneumonia-data\")\n",
    "\n",
    "TRAIN_IMAGES = os.path.join(DATA_DIR, \"stage_2_train_images\")\n",
    "TEST_IMAGES = os.path.join(DATA_DIR, \"stage_2_test_images\")\n",
    "PRED_MASK_DIR = os.path.join(DATA_DIR, \"stage_2_mask_images\")\n",
    "\n",
    "TRAIN_ANNOTATIONS_FILE = \"stage_2_train_labels.csv\"\n",
    "TRAIN_CLASS_FILE = \"stage_2_detailed_class_info.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:19.711511Z",
     "start_time": "2020-05-16T00:12:19.702514Z"
    }
   },
   "outputs": [],
   "source": [
    "class OpacityDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, image_dir, subsample, transform=None):\n",
    "        self.classes_df = pd.read_csv(csv_file)\n",
    "        if subsample is not None:\n",
    "            self.classes_df = self.classes_df[:subsample]\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Convert our classes to integers!\n",
    "        self.class_dict = {\n",
    "            \"Normal\": 0,\n",
    "            \"No Lung Opacity / Not Normal\": 0,\n",
    "            \"Lung Opacity\": 1\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.classes_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # Get the image, preprocess it for our model\n",
    "        image_path = os.path.join(\n",
    "            self.image_dir, self.classes_df.iloc[index, 0])\n",
    "        image = dicom.read_file(image_path + \".dcm\").pixel_array\n",
    "        image = image[::4, ::4]\n",
    "        image = image/image.max()\n",
    "        image = (255*image).clip(0, 255).astype(np.uint8)\n",
    "        image = exposure.equalize_hist(image)\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        opacity_class = self.classes_df.at[index, 'class']\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {\"image\": image,\n",
    "                  \"opacity_class\": self.class_dict[opacity_class],\n",
    "                  \"class\": opacity_class}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:19.719508Z",
     "start_time": "2020-05-16T00:12:19.712510Z"
    }
   },
   "outputs": [],
   "source": [
    "class OpacityMaskDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, image_dir, subsample, transform=None):\n",
    "        self.classes_df = pd.read_csv(csv_file)\n",
    "        if subsample is not None:\n",
    "            self.classes_df = self.classes_df[:subsample]\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Convert our classes to integers!\n",
    "        self.class_dict = {\n",
    "            \"Normal\": 0,\n",
    "            \"No Lung Opacity / Not Normal\": 0,\n",
    "            \"Lung Opacity\": 1\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.classes_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # Get the image, preprocess it for our model\n",
    "#         image_path = os.path.join(\n",
    "#             self.image_dir, self.classes_df.iloc[index, 0])\n",
    "#         image = dicom.read_file(image_path + \".dcm\").pixel_array\n",
    "#         image = image[::4, ::4]\n",
    "#         image = image/image.max()\n",
    "#         image = (255*image).clip(0, 255).astype(np.uint8)\n",
    "#         image = Image.fromarray(image).convert(\"RGB\")\n",
    "        image = Image.open(os.path.join(self.image_dir, self.classes_df.iloc[index, 0] + \".jpeg\"))\n",
    "        opacity_class = self.classes_df.at[index, 'class']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {\"image\": image,\n",
    "                  \"opacity_class\": self.class_dict[opacity_class],\n",
    "                  \"class\": opacity_class}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:19.749498Z",
     "start_time": "2020-05-16T00:12:19.720508Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_split = 0.2;\n",
    "\n",
    "opacity_dataset = OpacityDataset(\n",
    "    csv_file=os.path.join(DATA_DIR, TRAIN_CLASS_FILE),\n",
    "    root_dir=DATA_DIR,\n",
    "    image_dir=TRAIN_IMAGES,\n",
    "    subsample=8000,\n",
    "#     subsample=None,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "dataset_size = len(opacity_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "# np.random.seed(1)\n",
    "# np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Create samplers for data loading\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:19.754497Z",
     "start_time": "2020-05-16T00:12:19.750499Z"
    },
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define our NN\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, (3, 3))\n",
    "#         self.pool = nn.MaxPool2d((2, 2), 2)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, (3, 3))\n",
    "#         self.conv3 = nn.Conv2d(32, 64, (3, 3))\n",
    "#         self.conv4 = nn.Conv2d(64, 128, (3, 3))\n",
    "#         self.linear1 = nn.Linear(25088, 4086)\n",
    "#         self.linear3 = nn.Linear(4086, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = self.pool(F.relu(self.conv4(x)))\n",
    "#         x = x.view(x.size()[0], -1)\n",
    "#         x = F.relu(self.linear1(x))\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "    \n",
    "# class AlexNet(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(AlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         )\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(256 * 7 * 7, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(4096, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), 256 * 7 * 7)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# model = AlexNet()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T23:49:13.018378Z",
     "start_time": "2020-05-05T23:49:13.015379Z"
    },
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:19.760495Z",
     "start_time": "2020-05-16T00:12:19.756496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 10\n",
    "num_classes = 2\n",
    "batch_size = 8\n",
    "learning_rate = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:20.056406Z",
     "start_time": "2020-05-16T00:12:19.762495Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "feature_num = model.fc.in_features\n",
    "model.fc = nn.Linear(feature_num, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:20.061399Z",
     "start_time": "2020-05-16T00:12:20.058399Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = torchvision.models.vgg11(pretrained=False)\n",
    "# model.fc = nn.Linear(4096, num_classes)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:20.066397Z",
     "start_time": "2020-05-16T00:12:20.063398Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = torchvision.models.alexnet(pretrained=False)\n",
    "# model.fc = nn.Linear(4096, num_classes)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:20.074394Z",
     "start_time": "2020-05-16T00:12:20.067397Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    opacity_dataset, \n",
    "    batch_size=batch_size, \n",
    "    sampler=train_sampler,\n",
    "    num_workers=0\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    opacity_dataset, \n",
    "    batch_size=batch_size,\n",
    "    sampler=valid_sampler,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:20.080393Z",
     "start_time": "2020-05-16T00:12:20.075395Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_num_from_tensor(tensorItem):\n",
    "    return tensorItem.cuda().cpu().numpy().item()\n",
    "\n",
    "def get_numpy_from_tensor(tensorItem):\n",
    "    return tensorItem.cuda().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:12:20.088390Z",
     "start_time": "2020-05-16T00:12:20.081392Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_pred_sum(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim=1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n",
    "\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    return correct_pred.sum()\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim=1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n",
    "\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = acc * 100\n",
    "\n",
    "    return get_num_from_tensor(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T00:50:12.615000Z",
     "start_time": "2020-05-16T00:12:20.090390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.742 training_accuracy: 37.50\n",
      "[1,   100] loss: 0.767 training_accuracy: 37.50\n",
      "[1,   150] loss: 0.681 training_accuracy: 50.00\n",
      "[1,   200] loss: 0.633 training_accuracy: 100.00\n",
      "[1,   250] loss: 0.679 training_accuracy: 37.50\n",
      "[1,   300] loss: 0.666 training_accuracy: 25.00\n",
      "[1,   350] loss: 0.692 training_accuracy: 50.00\n",
      "[1,   400] loss: 0.659 training_accuracy: 25.00\n",
      "[1,   450] loss: 0.666 training_accuracy: 50.00\n",
      "[1,   500] loss: 0.700 training_accuracy: 50.00\n",
      "[1,   550] loss: 0.694 training_accuracy: 50.00\n",
      "[1,   600] loss: 0.637 training_accuracy: 75.00\n",
      "[1,   650] loss: 0.725 training_accuracy: 25.00\n",
      "[1,   700] loss: 0.709 training_accuracy: 37.50\n",
      "[1,   750] loss: 0.693 training_accuracy: 50.00\n",
      "[1,   800] loss: 0.614 training_accuracy: 50.00\n",
      "Finished training epoch\n",
      "Epoch [1] Training Accuracy: 66.312 Validation Accuracy: 57.812 \n",
      "[2,    50] loss: 0.654 training_accuracy: 62.50\n",
      "[2,   100] loss: 0.696 training_accuracy: 62.50\n",
      "[2,   150] loss: 0.679 training_accuracy: 62.50\n",
      "[2,   200] loss: 0.635 training_accuracy: 37.50\n",
      "[2,   250] loss: 0.682 training_accuracy: 62.50\n",
      "[2,   300] loss: 0.731 training_accuracy: 75.00\n",
      "[2,   350] loss: 0.625 training_accuracy: 62.50\n",
      "[2,   400] loss: 0.646 training_accuracy: 75.00\n",
      "[2,   450] loss: 0.578 training_accuracy: 75.00\n",
      "[2,   500] loss: 0.675 training_accuracy: 87.50\n",
      "[2,   550] loss: 0.621 training_accuracy: 62.50\n",
      "[2,   600] loss: 0.656 training_accuracy: 50.00\n",
      "[2,   650] loss: 0.600 training_accuracy: 75.00\n",
      "[2,   700] loss: 0.590 training_accuracy: 100.00\n",
      "[2,   750] loss: 0.630 training_accuracy: 62.50\n",
      "[2,   800] loss: 0.670 training_accuracy: 87.50\n",
      "Finished training epoch\n",
      "Epoch [2] Training Accuracy: 69.125 Validation Accuracy: 61.250 \n",
      "[3,    50] loss: 0.611 training_accuracy: 75.00\n",
      "[3,   100] loss: 0.605 training_accuracy: 75.00\n",
      "[3,   150] loss: 0.652 training_accuracy: 37.50\n",
      "[3,   200] loss: 0.589 training_accuracy: 62.50\n",
      "[3,   250] loss: 0.600 training_accuracy: 50.00\n",
      "[3,   300] loss: 0.620 training_accuracy: 87.50\n",
      "[3,   350] loss: 0.634 training_accuracy: 75.00\n",
      "[3,   400] loss: 0.670 training_accuracy: 37.50\n",
      "[3,   450] loss: 0.627 training_accuracy: 75.00\n",
      "[3,   500] loss: 0.614 training_accuracy: 50.00\n",
      "[3,   550] loss: 0.593 training_accuracy: 87.50\n",
      "[3,   600] loss: 0.585 training_accuracy: 87.50\n",
      "[3,   650] loss: 0.623 training_accuracy: 87.50\n",
      "[3,   700] loss: 0.603 training_accuracy: 75.00\n",
      "[3,   750] loss: 0.608 training_accuracy: 87.50\n",
      "[3,   800] loss: 0.603 training_accuracy: 87.50\n",
      "Finished training epoch\n",
      "Epoch [3] Training Accuracy: 70.609 Validation Accuracy: 65.250 \n",
      "[4,    50] loss: 0.595 training_accuracy: 50.00\n",
      "[4,   100] loss: 0.570 training_accuracy: 62.50\n",
      "[4,   150] loss: 0.600 training_accuracy: 87.50\n",
      "[4,   200] loss: 0.575 training_accuracy: 62.50\n",
      "[4,   250] loss: 0.599 training_accuracy: 75.00\n",
      "[4,   300] loss: 0.652 training_accuracy: 50.00\n",
      "[4,   350] loss: 0.590 training_accuracy: 75.00\n",
      "[4,   400] loss: 0.570 training_accuracy: 50.00\n",
      "[4,   450] loss: 0.584 training_accuracy: 75.00\n",
      "[4,   500] loss: 0.710 training_accuracy: 50.00\n",
      "[4,   550] loss: 0.657 training_accuracy: 87.50\n",
      "[4,   600] loss: 0.637 training_accuracy: 62.50\n",
      "[4,   650] loss: 0.582 training_accuracy: 100.00\n",
      "[4,   700] loss: 0.602 training_accuracy: 50.00\n",
      "[4,   750] loss: 0.605 training_accuracy: 37.50\n",
      "[4,   800] loss: 0.600 training_accuracy: 100.00\n",
      "Finished training epoch\n",
      "Epoch [4] Training Accuracy: 70.797 Validation Accuracy: 59.500 \n",
      "[5,    50] loss: 0.578 training_accuracy: 87.50\n",
      "[5,   100] loss: 0.572 training_accuracy: 62.50\n",
      "[5,   150] loss: 0.580 training_accuracy: 75.00\n",
      "[5,   200] loss: 0.598 training_accuracy: 50.00\n",
      "[5,   250] loss: 0.634 training_accuracy: 62.50\n",
      "[5,   300] loss: 0.547 training_accuracy: 87.50\n",
      "[5,   350] loss: 0.618 training_accuracy: 62.50\n",
      "[5,   400] loss: 0.564 training_accuracy: 62.50\n",
      "[5,   450] loss: 0.598 training_accuracy: 37.50\n",
      "[5,   500] loss: 0.599 training_accuracy: 37.50\n",
      "[5,   550] loss: 0.620 training_accuracy: 62.50\n",
      "[5,   600] loss: 0.578 training_accuracy: 50.00\n",
      "[5,   650] loss: 0.638 training_accuracy: 50.00\n",
      "[5,   700] loss: 0.571 training_accuracy: 75.00\n",
      "[5,   750] loss: 0.602 training_accuracy: 50.00\n",
      "[5,   800] loss: 0.556 training_accuracy: 62.50\n",
      "Finished training epoch\n",
      "Epoch [5] Training Accuracy: 74.672 Validation Accuracy: 61.062 \n",
      "[6,    50] loss: 0.566 training_accuracy: 75.00\n",
      "[6,   100] loss: 0.585 training_accuracy: 100.00\n",
      "[6,   150] loss: 0.598 training_accuracy: 87.50\n",
      "[6,   200] loss: 0.527 training_accuracy: 100.00\n",
      "[6,   250] loss: 0.532 training_accuracy: 75.00\n",
      "[6,   300] loss: 0.553 training_accuracy: 75.00\n",
      "[6,   350] loss: 0.581 training_accuracy: 75.00\n",
      "[6,   400] loss: 0.529 training_accuracy: 50.00\n",
      "[6,   450] loss: 0.506 training_accuracy: 87.50\n",
      "[6,   500] loss: 0.582 training_accuracy: 100.00\n",
      "[6,   550] loss: 0.568 training_accuracy: 87.50\n",
      "[6,   600] loss: 0.617 training_accuracy: 87.50\n",
      "[6,   650] loss: 0.514 training_accuracy: 50.00\n",
      "[6,   700] loss: 0.553 training_accuracy: 75.00\n",
      "[6,   750] loss: 0.544 training_accuracy: 62.50\n",
      "[6,   800] loss: 0.551 training_accuracy: 75.00\n",
      "Finished training epoch\n",
      "Epoch [6] Training Accuracy: 71.594 Validation Accuracy: 62.562 \n",
      "[7,    50] loss: 0.532 training_accuracy: 87.50\n",
      "[7,   100] loss: 0.518 training_accuracy: 75.00\n",
      "[7,   150] loss: 0.452 training_accuracy: 100.00\n",
      "[7,   200] loss: 0.582 training_accuracy: 75.00\n",
      "[7,   250] loss: 0.500 training_accuracy: 62.50\n",
      "[7,   300] loss: 0.533 training_accuracy: 75.00\n",
      "[7,   350] loss: 0.570 training_accuracy: 62.50\n",
      "[7,   400] loss: 0.527 training_accuracy: 62.50\n",
      "[7,   450] loss: 0.574 training_accuracy: 62.50\n",
      "[7,   500] loss: 0.558 training_accuracy: 87.50\n",
      "[7,   550] loss: 0.547 training_accuracy: 87.50\n",
      "[7,   600] loss: 0.583 training_accuracy: 75.00\n",
      "[7,   650] loss: 0.542 training_accuracy: 75.00\n",
      "[7,   700] loss: 0.543 training_accuracy: 87.50\n",
      "[7,   750] loss: 0.552 training_accuracy: 62.50\n",
      "[7,   800] loss: 0.520 training_accuracy: 50.00\n",
      "Finished training epoch\n",
      "Epoch [7] Training Accuracy: 74.031 Validation Accuracy: 59.750 \n",
      "[8,    50] loss: 0.438 training_accuracy: 87.50\n",
      "[8,   100] loss: 0.472 training_accuracy: 50.00\n",
      "[8,   150] loss: 0.485 training_accuracy: 87.50\n",
      "[8,   200] loss: 0.529 training_accuracy: 87.50\n",
      "[8,   250] loss: 0.548 training_accuracy: 62.50\n",
      "[8,   300] loss: 0.519 training_accuracy: 75.00\n",
      "[8,   350] loss: 0.524 training_accuracy: 62.50\n",
      "[8,   400] loss: 0.459 training_accuracy: 62.50\n",
      "[8,   450] loss: 0.510 training_accuracy: 62.50\n",
      "[8,   500] loss: 0.615 training_accuracy: 87.50\n",
      "[8,   550] loss: 0.518 training_accuracy: 87.50\n",
      "[8,   600] loss: 0.496 training_accuracy: 75.00\n",
      "[8,   650] loss: 0.548 training_accuracy: 75.00\n",
      "[8,   700] loss: 0.542 training_accuracy: 62.50\n",
      "[8,   750] loss: 0.534 training_accuracy: 87.50\n",
      "[8,   800] loss: 0.498 training_accuracy: 75.00\n",
      "Finished training epoch\n",
      "Epoch [8] Training Accuracy: 78.531 Validation Accuracy: 59.750 \n",
      "[9,    50] loss: 0.469 training_accuracy: 37.50\n",
      "[9,   100] loss: 0.457 training_accuracy: 100.00\n",
      "[9,   150] loss: 0.482 training_accuracy: 87.50\n",
      "[9,   200] loss: 0.505 training_accuracy: 100.00\n",
      "[9,   250] loss: 0.457 training_accuracy: 75.00\n",
      "[9,   300] loss: 0.518 training_accuracy: 100.00\n",
      "[9,   350] loss: 0.458 training_accuracy: 50.00\n",
      "[9,   400] loss: 0.436 training_accuracy: 75.00\n",
      "[9,   450] loss: 0.447 training_accuracy: 75.00\n",
      "[9,   500] loss: 0.500 training_accuracy: 75.00\n",
      "[9,   550] loss: 0.491 training_accuracy: 87.50\n",
      "[9,   600] loss: 0.465 training_accuracy: 87.50\n",
      "[9,   650] loss: 0.527 training_accuracy: 50.00\n",
      "[9,   700] loss: 0.536 training_accuracy: 75.00\n",
      "[9,   750] loss: 0.471 training_accuracy: 87.50\n",
      "[9,   800] loss: 0.480 training_accuracy: 100.00\n",
      "Finished training epoch\n",
      "Epoch [9] Training Accuracy: 82.672 Validation Accuracy: 59.750 \n",
      "[10,    50] loss: 0.448 training_accuracy: 87.50\n",
      "[10,   100] loss: 0.466 training_accuracy: 87.50\n",
      "[10,   150] loss: 0.400 training_accuracy: 100.00\n",
      "[10,   200] loss: 0.502 training_accuracy: 62.50\n",
      "[10,   250] loss: 0.454 training_accuracy: 75.00\n",
      "[10,   300] loss: 0.451 training_accuracy: 62.50\n",
      "[10,   350] loss: 0.429 training_accuracy: 62.50\n",
      "[10,   400] loss: 0.466 training_accuracy: 75.00\n",
      "[10,   450] loss: 0.491 training_accuracy: 75.00\n",
      "[10,   500] loss: 0.454 training_accuracy: 87.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,   550] loss: 0.477 training_accuracy: 75.00\n",
      "[10,   600] loss: 0.456 training_accuracy: 87.50\n",
      "[10,   650] loss: 0.420 training_accuracy: 87.50\n",
      "[10,   700] loss: 0.470 training_accuracy: 87.50\n",
      "[10,   750] loss: 0.451 training_accuracy: 87.50\n",
      "[10,   800] loss: 0.449 training_accuracy: 87.50\n",
      "Finished training epoch\n",
      "Epoch [10] Training Accuracy: 84.031 Validation Accuracy: 60.375 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_train_acc = 0.0\n",
    "    running_val_acc = 0.0\n",
    "    running_loss_full = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[\"image\"], data[\"opacity_class\"]\n",
    "        inputs, labels = Variable(inputs.cuda(), requires_grad=True), Variable(labels.cuda())\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 2000 mini-batches          \n",
    "            train_acc = multi_acc(outputs, labels)\n",
    "            print(\"[%d, %5d] loss: %.3f training_accuracy: %.2f\" % (epoch + 1, i + 1, running_loss / 50, train_acc))\n",
    "            running_loss = 0.0\n",
    "       \n",
    "    print(\"Finished training epoch\")\n",
    "        \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[\"image\"], data[\"opacity_class\"]\n",
    "        inputs, labels = Variable(inputs.cuda(), requires_grad=True), Variable(labels.cuda())\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss_full += loss.item()\n",
    "        running_train_acc += multi_acc(outputs, labels)\n",
    "        \n",
    "    for i, data in enumerate(validation_loader, 0):\n",
    "        inputs, labels = data[\"image\"], data[\"opacity_class\"]\n",
    "        inputs, labels = Variable(inputs.cuda(), requires_grad=True), Variable(labels.cuda())\n",
    "        outputs = model(inputs)\n",
    "        running_val_acc += multi_acc(outputs, labels)\n",
    "        \n",
    "    curr_train_loss = running_loss_full / len(train_loader)    \n",
    "    train_acc = running_train_acc / len(train_loader)\n",
    "    val_acc = running_val_acc / len(validation_loader)\n",
    "    \n",
    "    losses.append(curr_train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "        \n",
    "    print(\"Epoch [%d] Training Accuracy: %.3f Validation Accuracy: %.3f \" % (epoch + 1, train_acc, val_acc))            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T02:37:53.876036Z",
     "start_time": "2020-05-16T02:37:53.868039Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(losses, train_accuracies, val_accuracies), columns=[\"Loss\", \"Train Acc\", \"Val Acc\"])\n",
    "df.to_csv(\"resnet-high-contrast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T07:36:48.352066Z",
     "start_time": "2020-05-15T07:36:48.343070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.828125</td>\n",
       "      <td>73.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.390625</td>\n",
       "      <td>65.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79.859375</td>\n",
       "      <td>62.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.953125</td>\n",
       "      <td>73.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.937500</td>\n",
       "      <td>72.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train Acc  Val Acc\n",
       "0  84.828125  73.0625\n",
       "1  78.390625  65.0000\n",
       "2  79.859375  62.5625\n",
       "3  88.953125  73.6875\n",
       "4  90.937500  72.5000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(zip(train_accuracies, val_accuracies), columns=[\"Train Acc\", \"Val Acc\"])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T02:38:03.216042Z",
     "start_time": "2020-05-16T02:38:03.164059Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet-context.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T02:38:06.406020Z",
     "start_time": "2020-05-16T02:38:06.279060Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "# plt.plot(losses, label=\"Loss\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0,100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T21:17:38.469889Z",
     "start_time": "2020-05-15T21:17:38.327935Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([3, 512]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-70f7fba0c1be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'resnet18-10-32.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 847\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    848\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([3, 512]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('resnet18-10-32.pth'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T02:39:58.106218Z",
     "start_time": "2020-05-16T02:39:35.604430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred 0,label 0 770\n",
      "pred 0,label 1 493\n",
      "pred 1,label 0 137\n",
      "pred 1,label 1 200\n",
      "\n",
      "pred 0,label 0 {'Lung Opacity': 0, 'Normal': 387, 'No Lung Opacity / Not Normal': 383}\n",
      "pred 0,label 1 {'Lung Opacity': 493, 'Normal': 0, 'No Lung Opacity / Not Normal': 0}\n",
      "pred 1,label 0 {'Lung Opacity': 0, 'Normal': 42, 'No Lung Opacity / Not Normal': 95}\n",
      "pred 1,label 1 {'Lung Opacity': 200, 'Normal': 0, 'No Lung Opacity / Not Normal': 0}\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"pred 0,label 0\": 0,\n",
    "    \"pred 0,label 1\": 0,\n",
    "    \"pred 1,label 0\": 0,\n",
    "    \"pred 1,label 1\": 0\n",
    "}\n",
    "\n",
    "results_classes = {\n",
    "    \"pred 0,label 0\": {\n",
    "        \"Lung Opacity\": 0,\n",
    "        \"Normal\": 0,\n",
    "        \"No Lung Opacity / Not Normal\": 0\n",
    "    },\n",
    "    \"pred 0,label 1\": {\n",
    "        \"Lung Opacity\": 0,\n",
    "        \"Normal\": 0,\n",
    "        \"No Lung Opacity / Not Normal\": 0\n",
    "    },\n",
    "    \"pred 1,label 0\": {\n",
    "        \"Lung Opacity\": 0,\n",
    "        \"Normal\": 0,\n",
    "        \"No Lung Opacity / Not Normal\": 0\n",
    "    },\n",
    "    \"pred 1,label 1\": {\n",
    "        \"Lung Opacity\": 0,\n",
    "        \"Normal\": 0,\n",
    "        \"No Lung Opacity / Not Normal\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "for i, data in enumerate(validation_loader, 0):\n",
    "    inputs, labels, classes = data[\"image\"], data[\"opacity_class\"], data[\"class\"]\n",
    "    inputs, labels = Variable(inputs.cuda(), requires_grad=True), Variable(labels.cuda())\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    y_pred_softmax = torch.log_softmax(outputs, dim=1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n",
    "\n",
    "    correct_pred = (y_pred_tags == labels).float()\n",
    "    \n",
    "    for j in range(8):\n",
    "        pattern = \"pred %d,label %d\" % (get_num_from_tensor(y_pred_tags[j]), get_num_from_tensor(labels[j]))\n",
    "        results[pattern] += 1\n",
    "        results_classes[pattern][classes[j]] += 1\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(key, value)\n",
    "    \n",
    "print()\n",
    "    \n",
    "for key, value in results_classes.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 10 Epochs, 32 Batch, Resnet18\n",
    "* Validation values didn't increase that much past the first epoch\n",
    "* Need to check class imbalance \n",
    "* Need to check the amount of values of each class that I'm missing\n",
    "* Try other models with dropout \n",
    "* Clear indiciation of overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
